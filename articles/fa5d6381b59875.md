---
title: "検索エンジンをElasticsearchに移行するまでに起きたこと、やったこと"
emoji: "🌊"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ['Elasticsearch', 'laravel']
published: false
---
先日[Agent bank](https://agent-bank.com/)の求人検索エンジンをMySQLの全文検索からElasticsearchに移行しました。
それ自体は特に技術的に新しいことはありませんが、移行開始から一週間でベータリリース、本リリースまで二週間強とそれなりにスムーズに移行できたので、主にチームとしてどういう動きや判断をしたかという点に焦点を当てて流れをまとめてみました。

## 移行を決めるまでの経緯
移行を決めてからはスムーズに行けましたが、移行を決めるまでは正直お世辞にもうまくいったとは言えない状況でした。
反省の意味を込めてこの経緯もまとめてみます。

### APMによる負荷可視化とbi-gram化
去年ごろから社内で[datadog](https://www.datadoghq.com/ja/)によるモニタリングをしっかりやっていこうという機運が高まり、[APM](https://www.datadoghq.com/ja/product/apm/)をいろんなところに仕込んだことで、本番環境のクエリの実行時間などが詳細に把握できるようになりました。
これによりあからさまなn+1が発生しているようなクエリについては特定ができたため解消できたのですが、求人検索に関してはn+1などのシンプルな問題は特に見いだされず、特に全文検索が行われると非常に処理が遅くなり、RDSに最も負荷を与えているクエリになっているという問題が認識されるようになりました。
この時点で一回NoSQLへの移行も議題に上がったのですが、bi-gramでの分割検索でそれなりのパフォーマンス改善が見られたため、一旦NoSQLへの移行は見送りとなりました。

### 負荷テストと死の宣告
bi-gram分割で一時的にパフォーマンスは改善したものの、4月から始まったパーソルキャリア社との求人連携により求人数は劇的に増え続けており、システムがこのまま求人数が増加しても耐えられるかテストを行うことにしました。
求人数を二年後の事業戦略目標数値まで増やし、それに比例して選考データも増やすバッチを流し、[Apache Bench](https://httpd.apache.org/docs/2.4/programs/ab.html)でリクエスト数をあげながらどれくらいまで耐えられるか試そうとしたのですが、なんとこの状態では一回全文検索しただけでRDSが即ダウンするレベルの負荷が発生してしまいました。
このままいくと死んでしまうことが確定したため、抜本的対策を行うためのタスクを起票してバックログに積みました。これが8月中旬の出来事になります。

### リードレプリカへの分散化と失敗
その翌週、負荷テストの結果と、既にかなり高くなってしまっているRDSのスペックダウンを行うために、RDSのリードレプリカを用意して負荷分散を行うことにしました。
もともとAuroraのリードレプリカはmetabase用に用意していたのですが、metabaseは時々非常に重いクエリが飛ばされることもあり、サービス側と同居させることに懸念する声が上がりました。
これについては[Auroraにはカスタムエンドポイントが設定できる](https://dev.classmethod.jp/articles/amazon-aurora-custom-endpoints/)ため、これを利用してリーダーインスタンスを二つにわけ、それぞれカスタムエンドポイントで向き先を固定することでmetabaseの重さにサービスが引きずられないように対策しました。
[Laravel側は単純にconfigでライター/リーダーの切り分けができる](https://laravel.com/docs/8.x/database#read-and-write-connections)ということだったので、アプリケーション側はこれに任せて分散を行ったのですが、Queの処理がリードレプリカの同期より早く行われてしまい、何かしらwrite後にQueで行った通知などの内容が書き換え前の内容を元にしてしまっているという問題が発生してしまい、これらの問題を完全に解決するのは時間がかかることが予想されたため、一旦リードレプリカへの分散は断念することになりました。

### 突然の死とゾンビ化プロセス大量発生
リードレプリカへの分散を断念し元の状態に戻して週があけた8/29、システム全体が非常に重くなりました。
RDSの状態を見てみるとCPUが100%に張り付いた状態がずっと続いており、プロセスを見るとゾンビ化して死ねなくなった大量のプロセスが残留している状態でした。
どうやらクエリが一分を超えてタイムアウトしてしまうとプロセスが正常に終了できなくなってしまい、ゾンビ状態になってしまったようです。
これらのゾンビ化したプロセスを手動でキルし、RDSの設定でタイムアウト後にプロセスを殺すようにしたことでCPU負荷の増加は少し緩やかになったものの、クエリが一分を超えるほど長くなってしまっている状態は改善しませんでした。
クエリが遅くなっている原因は結局この日特定までは至れず、後日別の方法で負荷テストを行った際にやっと原因を特定することができました。

### 応急処置とスプリントゴールの変更
ゾンビ化が止まったことで負荷が青天井に増え続ける状況は回避できるようになったため、RDSのスケールアップで最低限サービスが使える状態に持っていくことができました。
それでもまだ軽くなったとは言えない状態だったため、APMでRDSがクエリの処理にかかっている時間を各エンドポイントごとに集計した結果を解析した結果、全マシンタイムの7割程度は求人検索の件数取得エンドポイントに食われていることが確認できました。
クエリ自体は件数取得も検索結果取得もあまり大きくは変わらないはずですが、件数取得は絞り込み条件を変えるたびに（間引きしてるものの）毎回フロントからリクエストが飛んでいるため回数が多いのが負担になっている原因でした。
お客さんに全文検索した状態で検索条件変えたりしないでくれというわけにもいかず、事業部と相談して一旦件数のリアルタイム表示機能を封鎖することにしました。
ここまでしてようやくタイムアウトなどせずクエリがさばききれる状態に持っていくことができました。

しかし言うまでもなくこれらの変更はコスト的にもユーザーの利便性的にも強い負担をかける暫定対応でしかないため、開発チームとして根本的対策を最速で行うことを最優先とすることで認識をそろえ、すでにスプリントが始まった直後ではあったのですがスプリントゴールを急遽「検索エンジンの移行を実現可能な状態にする」に変更しました。